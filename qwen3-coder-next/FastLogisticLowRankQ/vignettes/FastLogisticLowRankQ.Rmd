---
title: "Fast Logistic Regression with Low-Rank Approximation"
author: "Your Name"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Fast Logistic Regression with Low-Rank Approximation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

The `FastLogisticLowRankQ` package provides an efficient implementation of binary logistic regression that leverages low-rank approximations through Singular Value Decomposition (SVD) to handle large-scale datasets. This approach is particularly useful when dealing with:

- Large numbers of observations (n > 10,000)
- High-dimensional feature spaces (p large)
- Datasets with low intrinsic dimensionality

The algorithm is based on the Fast Binary Logistic Regression (FBLR) method described by Nurdan S. et al. (2023).

## Basic Usage

### Installation

```{r, eval = FALSE}
# Install from GitHub
# devtools::install_github("yourusername/FastLogisticLowRankQ")
```

### Loading the Package

```{r}
library(FastLogisticLowRankQ)
```

### Simple Example

Let's start with a basic example using synthetic data:

```{r}
set.seed(123)

# Generate synthetic data
n <- 200  # Number of observations
p <- 15   # Number of features

X <- matrix(rnorm(n * p), n, p)

# True coefficients (only first 5 features are relevant)
true_beta <- c(1.5, -0.8, 0.6, -0.4, 0.3, rep(0, p - 5))

# Generate binary outcome
linear_pred <- X %*% true_beta[-1] + true_beta[1]
prob <- 1 / (1 + exp(-linear_pred))
y <- rbinom(n, 1, prob)

# Fit the model
model <- FastLogisticRegressionLowRank(X, y)

# View model information
print(model)

# Make predictions
class_predictions <- predict(model, X, type = "class")
prob_predictions <- predict(model, X, type = "prob")

# Calculate accuracy
accuracy <- mean(class_predictions == y)
cat("Training Accuracy:", round(accuracy, 3), "\n")

# Show first 10 predictions
cat("\nFirst 10 predictions:\n")
cat("Actual:       ", y[1:10], "\n")
cat("Predicted:    ", class_predictions[1:10], "\n")
cat("Probabilities:", round(prob_predictions[1:10], 3), "\n")
```

## Regularization

The package supports multiple regularization options:

### L2 Regularization (Ridge)

```{r}
# L2 regularization
model_ridge <- FastLogisticRegressionLowRank(X, y, lambda_ssr = 0.1)

cat("L2 Regularized Model:\n")
print(model_ridge)

class_predictions_ridge <- predict(model_ridge, X, type = "class")
accuracy_ridge <- mean(class_predictions_ridge == y)
cat("Accuracy with L2:", round(accuracy_ridge, 3), "\n")
```

### L1-like Regularization

```{r}
# L1-like regularization
model_l1 <- FastLogisticRegressionLowRank(X, y, f = 0.5, lambda_ssr = 0.01)

cat("L1-like Regularized Model:\n")
print(model_l1)

class_predictions_l1 <- predict(model_l1, X, type = "class")
accuracy_l1 <- mean(class_predictions_l1 == y)
cat("Accuracy with L1-like:", round(accuracy_l1, 3), "\n")
```

### Elastic Net Regularization

```{r}
# Elastic net (combination of L1 and L2)
model_elastic <- FastLogisticRegressionLowRank(X, y, lambda_ssr = 0.05, gamma = 0.05, f = 0.5)

cat("Elastic Net Regularized Model:\n")
print(model_elastic)

class_predictions_elastic <- predict(model_elastic, X, type = "class")
accuracy_elastic <- mean(class_predictions_elastic == y)
cat("Accuracy with Elastic Net:", round(accuracy_elastic, 3), "\n")
```

## Advanced Features

### Custom Rank

You can override the automatic rank determination:

```{r}
# Force a specific rank
model_custom_rank <- FastLogisticRegressionLowRank(X, y, rank_override = 8)

cat("Model with Custom Rank (8):\n")
print(model_custom_rank)
```

### No Intercept

```{r}
# Fit model without intercept
model_no_intercept <- FastLogisticRegressionLowRank(X, y, fit_intercept = FALSE)

cat("Model without Intercept:\n")
print(model_no_intercept)
```

## Cross-Validation for Hyperparameter Tuning

The package provides built-in cross-validation functionality for finding optimal hyperparameters. Here's how to use it:

```{r, message = FALSE, warning = FALSE}
# Perform cross-validation
lambda_seq <- c(0, 0.01, 0.1, 0.5, 1)

# Using the built-in cross-validation function
cv_result <- cvFastLogistic(X, y, lambda_seq, folds = 5)

cat("Cross-validation results:\n")
print(cv_result)

# Plot results
plot(cv_result, main = "Cross-Validation Results")

cat("\nOptimal lambda:", cv_result$optimal_lambda, "\n")

# Fit final model with optimal lambda
final_model <- FastLogisticRegressionLowRank(X, y, lambda_ssr = cv_result$optimal_lambda)
cat("\nFinal model with optimal lambda:\n")
print(final_model)

# Compare with default lambda=0
cat("\nComparison with lambda=0:\n")
cat("Lambda=0 accuracy:", round(mean(predict(model, X, type = "class") == y), 3), "\n")
cat("Optimal lambda accuracy:", round(mean(predict(final_model, X, type = "class") == y), 3), "\n")
```

For more details on cross-validation, see the vignette 'cross-validation'.

## Real Data Example

Here's an example with a more realistic dataset:

```{r}
# Simulate a larger dataset
set.seed(321)

n <- 1000
p <- 50

# Create correlated features
X_large <- matrix(rnorm(n * p), n, p)
for (i in 2:p) {
  X_large[, i] <- 0.6 * X_large[, i - 1] + 0.4 * rnorm(n)
}

# Generate outcome with sparse true coefficients
true_beta_large <- c(2, -1.5, 1, -0.8, 0.5, rep(0, p - 5))
linear_pred_large <- X_large %*% true_beta_large[-1] + true_beta_large[1]
prob_large <- 1 / (1 + exp(-linear_pred_large))
y_large <- rbinom(n, 1, prob_large)

# Split into train/test
train_idx <- sample(1:n, 0.7 * n)
X_train <- X_large[train_idx, ]
y_train <- y_large[train_idx]
X_test <- X_large[-train_idx, ]
y_test <- y_large[-train_idx]

# Train model
model_large <- FastLogisticRegressionLowRank(X_train, y_train, lambda_ssr = 0.1)

cat("Training on large dataset:\n")
print(model_large)

# Evaluate on test set
test_pred <- predict(model_large, X_test, type = "class")
test_accuracy <- mean(test_pred == y_test)

cat("\nTest Accuracy:", round(test_accuracy, 3), "\n")

# Confusion matrix
cat("\nConfusion Matrix:\n")
print(table(Predicted = test_pred, Actual = y_test))
```

## Performance Comparison

For large datasets, the low-rank approximation provides significant computational benefits:

```{r}
# Performance benchmark
library(microbenchmark)

# Small dataset
n_small <- 500
p_small <- 20
X_small <- matrix(rnorm(n_small * p_small), n_small, p_small)
y_small <- sample(c(0, 1), n_small, replace = TRUE)

# Large dataset
n_large <- 5000
p_large <- 100
X_large_perf <- matrix(rnorm(n_large * p_large), n_large, p_large)
y_large_perf <- sample(c(0, 1), n_large, replace = TRUE)

cat("Performance comparison:\n")
cat("Small dataset (500 x 20):\n")
microbenchmark(
  fast_regression = FastLogisticRegressionLowRank(X_small, y_small),
  times = 5
)

cat("\nLarge dataset (5000 x 100):\n")
microbenchmark(
  fast_regression = FastLogisticRegressionLowRank(X_large_perf, y_large_perf),
  times = 5
)
```

## Conclusion

The `FastLogisticLowRankQ` package provides an efficient implementation of binary logistic regression with:

- Low-rank SVD approximation for computational efficiency
- Multiple regularization options (L2, L1-like, elastic net)
- Flexible rank selection
- Integration with CVST for hyperparameter tuning
- Support for large-scale datasets

For more information, see the package documentation and the original FBLR paper by Nurdan S. et al. (2023).

## References

Nurdan S., et al. (2023). Fast Binary Logistic Regression. *Journal of Computational Statistics*. Available at: https://github.com/NurdanS/fblr